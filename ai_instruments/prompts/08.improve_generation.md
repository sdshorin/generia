
(Тут перешел на Argument)

Помоги мне улучшить ai-worker. Я хочу, чтобы сгенерированное описание мира прокидывалось во все команды генерации.
Я уже начал делать часть работы  - сделал скрипт, который автоматически печатает схему ответа LLM и добавляет ее в промпт (и теперь вместо дублирующего описания, что именно должен включать в себя ответ, можно использовать этот скрипт вместе с {structure_description} - как это сделано в промпте world_description.txt. теперь я хочу переделать на эту механику все ролевантные промпты. Для удобства - я хочу красиво форматировать описание мира в отдельной функции и вставлять его с помощью одного шаблона {world_description} (а не как сейчас - прописывать кучу параметров отдельно.


-------
Сейчас у меня получается 2 схемы данных: одна - pydantic для строгой схемы ответа LLM, вторая - для загрузки параметров мира из mongo. Можно ли сделать более универсально  и использовать только одну схему? или это невозможно из-за вложеных объетов, которые преобразуются в json при сокхренении? интересует качественное и красивое решение, если для решения этой задачи нужно будет использовать много костылей - то такое решение мне не нужно, пусть все остается как есть


-------
проблема в том, что Optional не поддерживается в схеме ответов LLM. Может, просто унаследуем WorldDescription от WorldDescriptionResponse - так можно сделать? чтобы не менять старый обхект WorldDescriptionResponse, но через наследование  добавить новые поля, необходимые для mongo


-------
Отлично. Теперь перепроверь как описание мира сохраняется (я сейчас в коде вижу, что в mongo отправляется обект WorldDescriptionResponse вроде бы). И так же модицифицируй генерацию описания текста format_world_description - давай использовать  Field.description (и при этом - удалять из этого описания весь текст внутри скобок - так как этот текст передается только в llm для генерации мира , при использовании описания мира в других запросах эти уточнения не требуется. чтобы это было более явно - давай заменим скобки в description на квадратные)






-------
Помоги мне улучшить ai-worker. Я хочу поддержать генерацию миров неограниченного размера. Для этого мне нужно генерировать любое количество персонажей - но сейчас я ограничен размером ответа LLM.
Мне нужно модифицировать задачу services/ai-worker/src/jobs/generate_post_batch.py.
Я хочу, чтобы за раз генерировалось максимум 10 пользователей, и дальше задача вызывала сама себя - только с новыми параметрами.
В новой (рекуррентной) задаче - нужно сгенерировать на 10 пользователей меньше (они сгенерированы в прошлой задаче). Но, чтобы пользователи не повторялись, в промпт нужно добавить краткое описание уже сгенерированных персонажей.

То есть: 1. нужно добавить новое поле в services/ai-worker/src/schemas/character_batch.py - 1-3 абзаца с описанием сгенерированных персонажей
2. Нужно сделать новый промпт, типа "в прошлых {count_run} запусках было сгенерировано {count} персонажей. Вот их краткое описание: {description}. Новые персонажи должны быть уникальными и не повторяться с уже сгенерированными". Это будет отдельный мини-промпт в txt файле в директории services/ai-worker/src/prompts.
3. в промпте services/ai-worker/src/prompts/character_batch.txt нужно добавить этот мини-промпт через шаблон (при первой итерации он будет пустым)
4. в services/ai-worker/src/jobs/generate_post_batch.py нужно модифицировать код, чтобы он вызывал сам себя, если сгенерировано недостаточно персонажей. (нужно учитывать, что хотя в промпте написано сгенерировать 10 персонажей, LLM может случайно вернуть меньше- поэтому нужно перепроверять.) (то есть вызываются команды для каждого персонажа + команда для генерации еще N - 10 персонажей)
5. Нужно всегда проверять, сколько персонажей уже сгенерировано и сколько еще нужно сгенерировать - нужно смотреть на эти данные в mongo. 
(вынеси число 10 как отдельную константу)






-------
1. в промпте нужно указывать не только сколько уже сгенерировано персонажей и сколько осталось, но и сколько будет сгенероировано в будущем в других задачах (чтобыL LLM при генерации могла прикинуть распределение персонажей). (для посотв - то же самое)
2. Добавь дебажное значение - глубину рекурсии и максимально разрешеная глубина рекурсий (вычисляется как [количество персонажей] / 8 + 1). И максимально возможная глубина рекурсии в проекте - 50 для персонажей и 30 для постов (на всякий случай, чтобы из-за ошибки я не потерял все деньги при генерации)
3. перепиши код, отвечающий за распределение количества постов между персонажами. Внимательно подумай об алгоритме (нужно примерно раскидать посты по всем персонажам. LLM для каждого персонажа должен вернуть либо количество постов у него(так как может быть, что в мире один персонаж должен постить гораздо больше другого), либо вернуть весовой  коэффициент, отражающий количество постов у персонажа относительно других - и тогда этот коэффициент нужно будет рассчитать, с учетом распределения всего количества постов по всем персонажам). (нужно помнить, что LLM может ошибиться в расчетах )
4. Нужно так же прокидывать оставшееся количество постов для генерации между задачами в character_batch - чтобы корректно распределить посты по персонажам и сгенерировать ровно столько постов, сколько было в запросе.



-------
0. нужно сделать механику с кратким описанием уже сгенерированных потов для следующих рекурсивных вызовов (Эту механику мы сделали только у уже сгенерированных персонажей
2. LLM всегда возвращает валидные данные, но действительно нужно обрабатывать проблему если LLM вернула недостаточное количество персонажей или постов.
3. перепроверь, что начальные значения новых параметров в задачах правильно создаются обрабатываются


-------
Сейчас previous_posts_template и previous_characters_template заполняются так же при первой генерации, если батчей планируется больше одного. Это неправильно. давай сделаем отдельные мини-промпты, в которых укажем что в этот запуск генерируется столько-то там пресонажей из персонажей всего. И будем использовать их при первом запуске, если при генерации будет использовано несколько батчей

